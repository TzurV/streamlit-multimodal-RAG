 Hello everyone. We're starting right on time because this is the most exciting webinar we've had yet. So this is the only webinar we've started on time so far, but it definitely deserves to be this one. We've got an absolutely packed webinar, some awesome projects, some awesome people. And so we're going to have a really fun conversation. I'm going to hand it over to Charles very quickly. I just wanted to say, first of all, thank you everyone for joining and being interested. And then thank you also to Charles, Matt Yovi, and Trin Yu for joining and then spending an hour with us chatting about some hopefully fun topics. And with that, I'll hand it over to Charles. Nice. Yeah, thanks Harrison. And just to clarify for everybody asking in the chat, as far as I know Harrison is not operated by a laying chain agent he's operated entirely by biological I have seen him once in real life there is a real Harrison Chase yeah so with that let's kick it over to the people quickly making that harder to be a joke. And we're going to hear just five minutes on each of the three projects that we have here. So Shinhoo will be talking about his work on reacts, the sort of like fundamental prompting and control flow pattern that now powers a lot of agents in Langchain and elsewhere. We'll hear about the baby AGI project from Yohay, which has really sort of like captured people's imagination and sort of demonstrated just how far you can get with even like a relatively simple pieces composed correctly. And then lastly we'll hear from Matt on some experiments with code generation with agents. So I know the least about that third project, so I'm excited to hear about that. So yeah, Chenn't you if you could go ahead and I think you had some slides you wanted to share about the your work on react and elsewhere. Yeah, so I just have to go through a quick slide to share, but to motivate, you know, what they mean to do in React. So I was doing this project last year in the summer. And at the time, you know, it's like Palm just got reduced. GBC sphere has been there for a while. everybody's excited, chain of style just released. And the observation that I make is just that, you know, lunch models are getting better at two of the fundamental capabilities for task solving, reasoning and acting, but in a rather separate manner. So if you look at some of the reasoning work, like chain of thought, it's really generating those self-conditioned rhythmic traces to have some many different tasks, but it's not really grounded to any external environments. And on the other hand, at a time, you know, some exciting work including CECAN shows you can, you know some exciting work including C can shows you can you know connect bank models to external environments to external words by using this kind of grounding but they're not really using the reasoning capability or generating enough with interests. So the natural question that I ask is just can you combine both and like the leg motion generate both with interests and actions and that's basically what did you react. And I think the easiest way to think about this is that you have an agent and you want to do all kinds of autonomous tasks. And personally, you know, people do this kind of observation, action, kind of iteration. But the idea is really simple, right? So we just add a new action into action space and this action is called sinking. And the sinking thing can be any kind of thought that you have in your mind when you try to map from observation to action and it's good because first it give you like extra computation where you can use the salt kind of space to think about what you want to do and do this non-trigal mapping. And then it helps you generalize better because, like, based on recently you can generalize to a very different scenario, maybe it's sugars out, then you want to left. And finally, it's really a great alignment measure because you can directly understand why R. R. R. R. R. R. R. R. R. R. R. R. R. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A.R.R. policy is making all kinds of decisions. And you have a way to correct that into by looking at the reasoning, figuring out what is wrong, and then change the policy by just changing the reasoning, which is really hard for traditional error. I think the beautiful thing is that there's also a symmetry, right? So action is also a way to augment because we know something something that Chad TibT is terrible as some of the halluciation problem because even the largest model in the world can now like exhaust all the knowledge in the world so there could be out out there could be negative knowledge. And you have to enter with environments like Wikipedia or Google or external large business to really get a chance to augment reasoning was up to day knowledge. And I just want to quickly share what the original demo looks like. So it's a very poor looking kind of a digital notebook compared to the fancy mentioned. But you know, one of the oldest kind of script like that and basically you define you know what is a lunch model with open AI maybe and you define, and the environment is kind of like opening a gym where you have, you know, step and like give you observation. And the prompt looks at something like this, right? So you have like an instruction and you have some trajectories of you know what kind of human way of solving the task looks like and you can have a few shot trajectory and then you just do this kind of like sought action observation loop and give a very quick example so here is a question right so how many championships have period it's less than the brown James and the way rest of this you know it's like it searches Wikipedia, and then it tries to, you know, use presenting to extra information and then chance, something like a shelf-star reasoning, it's not really able to solve it. So if you give it a try. Uh, should you, if you could just zoom a little bit, some folks on lower quality connections, I think think are having trouble seeing some of the text. Oh, okay. Let me try my best. Yeah, you do it. Yes. So I just want to show this limitation of chain of thought, which is, you know, because it doesn't really know Steph Curry has another championship in 2022. So he thinks Steph Curry has three championships and the Brown has four. So Steph has less championship than the Brown James. And that's a typical example of like information I'll do this and yeah I think I'll stop right here. Shenn you I have a question for you so like the kind of like thought action, thought action observation kind of like sequences used a lot of places now including in LinkedIn very heavily. How like how how did you come up with that kind of like prompt engineering flow and those keywords and did that require a lot of like experimentation or did you try to just like try that once and it kind of worked? I think it's more of the latter situation so I think maybe there could be better from engineering to make it work better. So one thing I still haven't figured out is you know it's better to have those strict, you know, sought action observation loops or is it better to just put sod as like a special action called thinking action? There is a name being, you know, if you look at that mention a lot demos, many of the time you don't really need sauce in some of the intermediate actions. You maybe need salt only sparsely. So that thing I haven't figured out. Some of you might be needed to. Yeah, I mean, so that's another question. How did you kind of like evaluate this? And I'm assuming this came out as a paper and so I'm assuming there was some quantitative metrics and so yeah what like what are those metrics do you think they're good and are there any others that you're excited about? That's a great question. So in fact, I think the hardest part about my paper is try to come up with tasks to evaluate. Because if you look at the traditional NLP task, they're not really meant for this kind of autonomous interaction kind of thing. So what people typically do is that for something like question answering, right, you have like a very specially trained retriever to overfate to that task and then you have like an answering module specifically fine tune to that task. So it's becoming like a lambering game and also I think on the other hand you have those r-ril tasks but it's very hard to you know prompt you know things with kind of IL so the trade-off I chose to reasoning tasks and to our task and our tasks are kind of like a text game kind of version, except real robotics or video game. And for the recent task, it's like a question answering task and an effect verification task. I think the surprising thing is that on the R.O. It's really great performances. So our performance is better than we're using only one shot or two shot prompting. The performance is much better than you know imitation learning stuff are using like a hundred thousand examples. So at the time I know there is something about this. Yeah, so there's lots of questions coming in that we could cover on React, but I want to make sure that we get to see all the projects before we dive into D-B-Bahn. Any of them, there's lots of consonances and connections to make. So I think we'll kick it over to Yo-Hay for just a couple of minutes to talk about what's up with baby A. G. I think this is actually the first time I'm like talking about it like it's all tweets. So if you follow me you've probably heard this but I'll just do a quick story is that I was looking at the whole Genesis of Baby Asia I was looking at the hustle GPT project where people were using chat GPT as their co-founder and I was fascinated by it and I wanted to do it but I didn't have the time so I asked myself can I cut the human out of it so what can I just make an AI founder start a company for me? And I made like a prototype on Jupiter for right having to write all the code shared a prototype. Really I just said this is you know like my AI founder is kind of working. And then my friend Jenny was like, oh dude, just make baby AGI. And then that's just kind of caught on and it's just like everyone started calling him that. So I was like, all right, well, people are excited about this like let's let's just let's do a research paper apologies it's not a real research paper I simply fed the code to gpity for and said please write me a scientific research paper and that was the output so it's absolutely not an actual scientific research paper, but it seemed in line with what I was seeing in the market. And then the reaction continued, and I saw auto-GPT and I thought if this is out might as well open source my code. I like simple codes so I really paired it down to what 105 lines of code and I took the original nickname that Jenny gave it, Baby AGI, and released it on GitHub. And it's been pretty wild. So let's this backstory. I can explain how it works. The way it works is, you know, it's actually the same way I work, which is that you have a task execution agent that just does the first task on the task list. Then at the end of a task, there's a task creation agent who says, okay, you finish this task, what other tasks should we do based on that result? And then a task prioritization agent who basically re-ordered the task and sends the first task to the task execution agent. And I designed it that way because that's exactly how I work, right? I start the morning, I jump on my to-do list, I start on the first one, and I just execute. If a new task comes up, I add it to my to-do list, and I just keep executing at the end of the day, I go through my task list and I re-priotized so the next morning I can wake up and I just start at the top. Again, I know that's very systemic in terms of the way I work, but I literally just map that into the Asian. Yeah, I think one of the things that immediately like jumps out looking at the generative agents work that just came out from Google and Stanford and at baby, GI and others is like this problem of sort of like prioritization and like how do you interleave sort of like meta thinking about which tasks you should complete with just the actual like completion of tasks. So it sounds like like quick cursory overlook of baby AGI it looked like the primary thing was this like task you for management but it sounds like you mentioned to sort of like before bedtime reprioritize kind of thing so how does that yeah I mean for for me you know as a human right I don't have the same cycles that that AGI does. So my task prioritization agent only works at the end of my work day. But that being said, I'm sure I do it sometimes, right? Like if I get an email, if I get an email from an important limited partner I might respond to that email before I do something on my task list. So I think there is that, you know, and that goes into the where do you create an input, right? Like can you listen for incoming emails or messages and then create tasks based on that, which wasn't implemented but was always part of the initial diagram? The part that jumped out about baby AGI to me compared to like the kind of like react style prompting is like it is exactly this task prioritization list and my sense is that this probably helps with like more complex tasks where you want to like plan it up front and then kind of like offload stuff as opposed to maybe with like simpler things like even two hop things you can kind of just do it on the fly is that the right right way to think about it? Have you like noticed any like have you compared the two approaches in any kind of like way and and yeah noticed differences in terms of where they're good, where they're bad. I think what you said is right, right? I think with more complex tasks having a task this ahead of time probably is more, it makes more sense than kind of just guessing one by one, right? Because you do need to plan. And at least in this case, the objectives for baby AGI initially when I said it were not like there was no finit end to it, right? It was like grow a company, start and grow a company, right? So by design, there was no Finit end, which means that you have to keep creating tasks, right? As a founder, there's always things to do. So that's a by design that's different. But as people start using it to create objectives, it does seem like a lot of the experiments are around beefing up the task list. So you can kind of do serialization or subtask or milestones. But I do think that's where a lot of people are paying attention to. I wanted to pull one question from the Q&A before we kick it over to Matt to hear about the code generation work because it felt quite relevant to what you and Harrison were discussing which is like what are you in Harrison we're discussing which is like what are the kinds of tasks that you've seen that baby AGI actually fails on or struggles with and maybe some like suggestions of where things might improve. I think people see cool Twitter demos all the time which are generally the things that work, but there's this sort of like negative results problem. So it's harder to know like where are these things failing and that's where you know the good ideas for what to do next come from so any thoughts you know hey the first thought that popped in mind is it's it's not good at doing the things that people wanted to do like that that's the people where people's imagination goes because it's really early right it's just a framework an idea people immediately jump to what it can do it's's far from the first thing that people imagine. Just because there's a lot of work and getting the tools to work together and all of that. This might be a kind of a cop-out answer about what it's really good at is is getting people excited about building I think You know, I think it's a it's invigorated a lot of people a lot of people who weren't building jumped in to build, people who weren't sure what to build next jumped into build after seeing these projects. So again, that's not really the problem that the code solves, but that's my observation of what it's been where it's really succeeded. But on the specific tactical, right, I think with a I have one baby a ji version with a Lang chain that the most complex task it did was I asked it to create a folder in my Google Drive with a document describing each of the main friends characters and it was able to do a web search, create a Google Drive, drop six documents in it, and then add one bio to one of the six documents. And then it cropped out. So, you know, getting there. Not bad. Yeah, I've definitely worked with people who could do less. All right, so thanks for that. And we'll kick it over to Matt to hear a little bit about what he's been working on. You can give a better introduction to your project than I can, so go ahead and take it. Yeah. So I think the GPT 4 in particular is the reason why we're seeing this explosion and like GPT 4 and Yoh hey combined to create this this ability for the rest of us to imagine what's now possible is is really how I'm viewing this moment. That the step function in, you know, between 3.5 and 3 to 4 is so huge. I mean, not just going from 4k to 32K, but it actually like pays attention to almost all of the context. Like everybody remembers JPT 3. If you stop 4K and you're probably doing something wrong because it was never going to do what you wanted it to do. And you know, don't get me wrong, 32KGPT4, if you try to stuff 32K tokens will also start to get a little bit confused too. But now if you stuff 10, 15,000 tokens, it really does pay attention. The second thing, and I'm not sure whether the existing models have really fully taken advantage of this yet, is that it follows the system prompt for instruction following incredibly well. In fact, I've heard it almost described as it follows it almost too well. Okay, so what does that allow us to do? That allows us to actually define a protocol that we can have a reasonable expectation of of GPT for to follow when we're essentially embodying it with the capability to take in inputs and then perform actions in the real world. So the project that I want to share is very very simple and it's kind of a thousand feet you know backup from what you know hey is is is done and I think you know the the thing that I really want to come out of this is you know hey like your project and my project need to meet in the middle somehow so that we can have you know some killer baby AGI code agents and And that's a project that I'm just calling Yamel Runner. And Yammer consists of a little application that runs these Yamo scripts that are defined in a protocol that you put into the system prompt. And I can just kind of show you guys what I mean if I can share my screen here. In the meantime, there's a Q&A button right next to the chat. Should be on the right hand side of your screen. You can put questions there and vote on questions that you want to hear the speakers discuss. And then, Matt, you might want to zoom in a good bit on that one. Little hard to see. I think a mat's lang chain agent cut that. Broke. Yeah, we was in the 32K context limit. That's my guess. Yeah, that's what it was. He's trying to stuff it too much. Yeah, well we see what's happening there, let's see there's a question, highly voted question from the Q&A that I'd love to hear Yohay and should you talk about, like we're starting to see sort of like patterns of how agents work, both at the level of like, you know, low-level prompting techniques about like thought, observation, action, and then also sort of like control flow techniques and augmentation techniques so like have a memory stream that you can access that's like stored in a in a stream-oriented database or you know these like control flow things around when when to recursively improve and when to like when to stop and act. So I'm curious what kinds of patterns are emerging that you've seen that are maybe surprising or people might not have heard of. And of course, Colin wants to know who's going to write a book on this so that we can all just buy the O'Reilly book and know what the good patterns are. I think I mean I think that's what's exciting is that right with with with Asia being so simple people are taking at a lot of different directions so some people are working on having agents talk to you like having the task execution agents talk to each other or having two of the baby guys talk to each other or separating tasks into sub-task and then spinning up another chain that just handles that sub-tasks and reports back to the main and just like more task manager just more complicated organizational structures. I've seen so many, but those are some of the ones that popped mind in terms of, yeah, just seeing more agent patterns. I think you're muted but I think for example you're muted yeah I think for example you know the self refinement or self-credits thing, I think is quite exciting. So you can actually look at what you have done and reflect what you have done wrong and improve it. I think, in fact, it almost feels like you know in the traditional oil framework there's a lot of like individual modules where you have the credit you have you have like a planner you have a controller and stuff and it seems like we can already start to replace everything with a different large number model and then start to build very general things around that. Do you think like maybe is there a limit to how good you can get with just self criticism like it feels a bit like cheating? I know I have to see a lot of successful results before I really believe that self criticism could work as well as it does. And now I feel like the question is like does it just keep working until these things are like perfect. I think obviously there is a limit but I'm not sure if we have reached that yet in the sense that you know people are very eager to publish whatever is you could be published first and then try to get the results already so I don't think people have carefully examined all the possibilities and explore the limit yet. But like everything in mention, I mean, I feel like I still feel like we still haven't explored the limitation of prompting yet. So I'm really very about this. We've been thinking about this a bit at Lane Chain as well because obviously with all the new types of agents coming and we've been trying to see like what kind of like the differences are between the react style stuff that we have now and what are the new pieces and what are missing and I think from and so I think we'll probably write something about this because I think this is a really good question that everyone's kind of what wanting to know and I think like I think in addition to the react style promptly the new things that I've seen are I think the the baby AGI introduces this like planning and execution kind of like step and separates that and I think that's something novel and I think that's something new and I think that unlocks some like new longer run things and then I think also compared to again like the react style thing is all kind of like in buffer memory and that kind of like runs out of space when you get like longer running things which baby agi you know I think it's also different tasks that they were designed to do right like I think the reacts out prompting was designed for a lot of these like multi-hop kind of like two-hop question answering the baby AGI is designed to like you know solve world hunger or something like that and so they of course they have different kind of like memory constraints and how you construct the buffer and then the third thing that I think and this kind of combines some of like the this kind of gets to like the planning execution but just more generally like yeah the idea of I forget who said it on Twitter about like stacking of language models and just suffering it out like you do this task you do this task that's fine and not have one big thing and so I think those are three kind of like recent trends that I've seen between. I think baby I, GI does a lot of those. The auto gPT is the other kind of like big one. I think that's mostly just like taking advantage of GPD 4 plus this long-term memory part. I think they add in that kind of like unique and novel long-term memory thing with the vector store retriever. But yeah, echoing kind of like what you always said it's yeah, there's a lot of there's a lot of cool stuff going on and it's just the beginning. So whoever writes a book now will have to be updated in about a week. Yeah, it looks like we got Matt back. If you want to maybe do, see if you can share your screen again. We'll give it one more try. Sorry, I live in upstate New York. My internet connection was like a third world country. All right, here we go. Okay, so the basic idea here, I'm a good bit more there because like, yeah, like maybe 200% or something. And is this good enough more? Yeah, I'm going to say a lot more actually. Yeah. Oh, okay. Yeah. Okay. Yeah. Okay. Okay. Got it. Yeah debate I'll keep this super simple so that we can get back to you know the general discussion but the view that I have is it essentially you know the really practical thing right now is what I'm just calling pseudo-agents which don't do quite as much on their own as you know we're asking baby gee PT or honor gee to do but really allow us to work effectively offload a bunch of work to the large language model but really be part of the loop and the decision-making process. And I think that there's going to be like a ton of U.S. that goes into this, you know, in the next probably couple of months, but of course, every single time we make a time prediction, it gets beat by half these days. And so that the basic idea is that we give it this protocol, which I have an example of, called Yammer, which gives a very instructions to follow as to how to take directives from the user and then turn it into a sequence of Yamel Runner instructions and so so here's a how I'm using this so first of all you know you want to put in as much context as you can for code at least up until the point where you don't want to confuse GPT4 or run into context window limitations. And after that, you can see that, you just ask it to acknowledge and understand that it's Reddit. And then I just did a really quick example, which is, you know, what's the easiest feature that you could add? And it decided that the easiest feature that it could add would be to display a version. And so it creates this Yamel runner file and you know that includes a couple of very very simple operations and you know Yammer runner can execute commands it can edit files it can add files to file operations much like you know a lot of the GPT agents. And we can generate a new sequence of instructions and we can execute this and it will do all of these things. And I think that what's really interesting now is that like, GPT4 has clearly been trained to do co-generation problem solving within code and troubleshooting within code. So that actually lends it very naturally to the use of GPT for as a co-generation agent. The trick though that I've found is that you really need to treat it like a dance. It really needs to be something where sometimes gPT4 is leading the dance and sometimes you're meeting the dance and and and that's why I don't think this can be quite automated to the extent that you know we can fire and forget baby gp t and and expect that to work and that's where I expect the UX to really come in and fill this gap to make fast iteration possible because this really is truly enabled by by gPT for now. So, Matt, when did you start working on some of this stuff? So I know we chatted for the first time back in October, November, something like that. And so I guess like, yeah, when did you start working on this and you've touched a bunch on, yeah, how GPD4 is enabling a lot of this now, but like, you know, what were the biggest differences between working on it back then and now and how, yeah, and does that hold any kind of like clues for what it might look like in the future and I also think we're zoomed in on an empty stage so I will try to fix that. Yeah so I started playing around with uh, GPT 3, DaVinci Double II code agents in the summer last year and what I found is that it wasn't very useful yet for a number of reasons. And I'm sure a lot of people, everybody in this chat and a lot of people, ran into the same sorts of things where and we're running into many of the same problems now only we're just able to solve more sophisticated and complex problems. But Cognosis Shift with a module called I JavaScript, which was a GPT3 agent that was embodied using a node repel that was put in a loop. It did automatic summarization of the various steps that had had taken. It had to also frequently remind Da Vinci Double II what it was doing and what at last did because it would very easily lose track of that. The bigger and the larger models have certainly, you know, just expanded the level of complexity that they can deal with. I think that the real challenge today remains in two different categories. The first is context, of course, and context management, not just context length, but context management, where even if you have 32K or we get 640K, you know, next year, hopefully, who knows. And we're still going to run into the limitations of what we want to be able to put in memory. So we're still going to have to do context switching and essentially do what the virtual memory machine does in an operating system, which has swapped pages in and out depending on what the process needs. The only problem that we have that kind of extends past that is there is a deterministic way of knowing what pages to swap in and swap out from the operating systems perspective because the process knows exactly what it needs to and ask very specifically for the files or the pieces of memory that it's looking for. And so we actually have to come up with some kind of query planner, I think, to really solve this problem. I think we're a long way away from solving this problem. Again, a long way away from solving this problem in 2023. Who knows what that means it might mean next week. I think the second problem we have to solve again is a lot on the UX side where these things really aren't going to be able to provide a lot of useful work until we get the UX dialed in so that we have this tango between the agent and the user. And my name with this project is just to kind of create some kind of protocol that can be used to play with different baby AGI style projects on one side and play with different UX's on the other side so that we can kind of create an embodiment standard essentially that we can really kind of figure out what that U.S. looks like, you know, and really lean into the fact that, yeah, of course it's not a, you know, going to be a fire and forget. But humans aren't fire and forget. It's not like you can have a software engineer working on a project and just say, you know, one sentence, here you go, go do it, go build that feature, anything of any complexity at all, it's going to be a dialogue. So I'm really interested in that because I think everyone's a lot of people are thinking about the right UX for these things not not agents but just generative in that AI in general are. And I think the main one that's emerged is chat. And so like, is chat a good UX for this and baby AGI specifically is not really a chat thing and so like yeah how do you guys think about that? I mean I can speak for myself right I think the biggest challenge with chat gPT or chat interface is that it only works as long as you're there and and what I want is for stuff to get done when I'm not and so the user interface that I want out of my autonomous tooling is, you know, I pretty simple, I don't know, like almost like an API probably, right? Like I want to be able to like, you know, flag a task list, you know, for my to do, to do, you know, my task manager and just say, you know what, let's send this to my age, you know, baby agi and then just have it kick into it. But going back to how far away are we from things being useful, I think the most earlier use cases will be non-generic, like very specifically tailored autonomous agents where you craft a prompt for a specific use case, that's going to be pretty quick from where we are. I think turning baby agent to something that can do all of the tasks from any prompt is going to be a lot more work. But I think in the short term, building out these like very simple, straightforward autonomous agents and plug it into your workflow seems like something I see myself doing in the next few weeks. Chinu, I'm curious, as somebody who's like more on the research side, to an extent you're like thinking about or participating in conversation about UI and UX. I've noticed there has been some really great work coming out of Google research going all the way back to like AI chains and prompt chain are like on this kind of stuff so I'm curious if you've thought about that have anything to contribute from the more research you side. Yeah I just want to continue on UHUH's point. I think, like, to me, there are just two different types of interaction. So one is kind of automatic, and when it's interaction with human. So if you think about you know interaction with code execution or interaction with web pages or digital digital environments etc etc or even like with another agent versus you know when you interact with a human through a dialogue. I think the fundamental difference is that for the lab for the former cases you get the feedback in an automatic manner versus if you interact with a human, you have to wait until the human reply and type being the same for you until you get that human feedback. And in some sense I feel like human feedback is a double-edged sword in the sense that that's probably the strongest alignment measure that we have. You know, the matter is RLHF or, you you know aligning the model with us through a dialogue. That's the biggest alignment measure for us. But that's also very hard for the developers or researchers or if you really want to do something you can't afford to be someone like opening AI and do our OSHA for everything. So that's actually the motivation for another of my work called Webshop. And the idea is that how can you build like a complete task? Not just, you know, something opening like baby agile, but you have an instruction. You have an environment, but most importantly you have a way to evaluate the model and give that feedback to the model automatically. But I guess, you know, from the research perspective, we need to do both, we need to push the frontier of how autonomous agents can be. But on the other hand, we also need to synergize those autonomous agents and plug it into our human interaction. And I say, you know, there's no reason not to do both. Nice question. I was going to add one more thing on top of it if you don't mind. On the UI I do want to set it and forget it but I agree that as tasks get more complicated like I do want to set it and forget it, but I agree that as tasks get more complicated, I do want to be in the loop. And it was a super lightweight demo. I think I used Zapir NLA, and I actually used Zapir's text, but I crafted the title to be like text boss if you're not sure of answer. So what happens is if it doesn't know the answer or if it has like a list of five and he has to choose one, it actually sends me a text message asking for it. And the goal would be to have a listener in the task creation agent so that whenever I do respond, it'll then continue with the task. I think is the UI that is ideal for me, because if I have autonomous agents, I want to be able to just text back and forth. And so you did you explicitly add that as kind of like a do you explicitly add that as a tool or did you kind of like tell it if none of these tools work then do this? I created a tool called you know ask the boss if you're unsure about decisions and I don't think that's the right one, but it works well enough that I think it's worth, you know, if you can if you tweak it, it can get better. Yeah, that's so like. What's up? The E to be project does something very similar I think where it has it has a couple of things that that it will expose as tools. One is to prompt the user and the other is to ask it to make a decision, which is kind of neat. I haven't seen it actually in action, but I think that's probably the kind of interactions that we're going to need to have with it to your point. And on that one like interesting thing that I saw like completely anecdotally when working with some agents in Lincchain is that like when you give it explicitly a tool to like ask it does much better than if you're like if you don't if you can't use any of these tools like then respond directly to the user like if you give it explicitly a tool it's just much better for whatever reason and so I think yeah whether that's a text or like a input thing and if you're in like a Jupiter notebook or something else like I yeah I've seen that be it basically be the best strategy for getting that type of thing which I think is just interesting because I did want to bring in one of the questions from the Q&A that had a ton of up votes and is relevant here which is in particular we've been talking a little bit maybe about like a single system primarily here whether it's dialogue or an autonomous system. And one of the questions in the chat was about developing and managing large numbers of agents, maybe a bunch of task-specific individual baby AGIs who only care about one particular thing. So do you think that that things change when you scale it up to like 10 or 100 autonomous agents? What does that look like? Does it look like a Slack channel? Does it look like an MMO video game? Does it look like your email inbox what do folks think? And there's that paper recently that came out I think like Monday or something like the Similakra thing which seems very related to this. Have any of you guys read that in detail or thought about that? That paper is pretty wild, including the, so this gender of agents paper, it's from the Center for Research and Foundation models and Google Research they created like a stardu Valley type system where everything was controlled by GPT 3.5 turbo and they were able to like at half or a quarter real time simulate like 25 agents who had like a memory the ability to reflect on that memory and adjust their personality over time and to interact with this simulated environment and a lot of the pieces the pieces are all very familiar. They look like reasoning, observing, reasoning, and then acting. They look like a memory stream that you can read from and write to. They look a lot like, in fact, they use Jason and like use Jason as a communication interface. So they have pieces, there are pieces that we see reflected in each of the three projects here in that one. And it does seem like if you gave, if you made an entire little Stardew Valley Town whose job was to help run your startup, that does seem like a potentially very good interface for a group of baby AGIs. So something that you guys may or may not have also seen is the work that Merrick Rose's group is doing on this. They've been experimenting with LLC-based NPCs for a number, actually I think since last year. And I saw an early demonstration of it last year and was like, yeah, it worked pretty well, but like that they're starting. I think they just released something this week or last week that looks really, really good and represents an advance that I think is consistent with the new types of models that we're seeing. So check that stuff out too. Another paper that, and I'm not sure which group this is, but there's like the Camel paper that I'm not sure which group this is but there's like the camel paper that someone implemented kind of like in like chain yesterday I think which is basically just it's just two agents but it's talking to each other and they each have kind of like their own personalities and their own objectives and you can kind of see the conversation kind of like flowing and I think it was more kind of like experimental than anything else but I think mean, you know, it seems like there's kind of like hypeways every two to four weeks and I think like, you know, the past two to four weeks have definitely been about baby AGI and auto GPT and so I wonder if like the next two to four will be around like yeah multiple agents talking to each other and I think there's a few different varieties of that one is like kind of like these simulation style things but then I do think there is another variety which is just this like stacking style thing where there's a more controlled flow and there's kind of like a router which is using them as different and I see them that's slightly different right like one's kind of like simulating a real world situation and then the others just like I don't know trying to do a task in the tools it has or just happen to be other agents themselves and so I'd maybe draw like a little distinction there, but yeah, still so really, really fascinating to see what comes out. I actually like the question about the front end though, I do I mean cost is important right like as you're running if I'm running a whole bunch of agents to run a company like cost is gonna pile up where it gets stuck like I would definitely want like a like a simple log of all the stuff they're doing. Again, I'm coming from the, if I'm running the company and having a whole bunch of agents running it like for me I want like a gusto right? All of these are you know AI employees. I want to know how much I'm paying, I want to know how much work they're doing, and I want to know if they need attention. Yeah, this is another question that we got for the Q&A from David Lou, like humans. We have, you know, we know that we have limited resources. We're trained how to like use those resources effectively, our time, our energy, our money. And one thing that language models don't currently have is any notion of those limitations. Like they have learned our patterns of thought our patterns of tool use and so you know only text the boss when you're unsure is maybe something they can pick up quickly but that's not the same as their actual resource constraints, right? So the question is like how do we get them to behave according to the constraints that are actually in operation for them? Yeah, I want to quickly mention, you know, like this thing has been considering cognitive science as like matter recently, you know, since that I can, you know, spend 10 minutes thinking about this problem or I can spend an hour thinking about this problem, but I need to consider the constraints and spend the outcomes and do the spare computations right so if I only have five seconds then I did you have a very different strategy then I have an hour and it's really like a weird loop because in some sense we're doing this matter recently pretty well and it's still not very easy and I think maybe now it's time to consider this problem again in the sense that you know you can have like a matter of reason to say okay if I if I only have this kind of money and I know, you know, this token cost this money, can you come up with like a prompt that is less than this lens and something like that may be possible to study already. Yeah, I mean there's a comment in the chat that says like add a budget keeping agent and it's kind of a joke but kind of serious and I think this comes back to like you know I think this comes back to like, you know, I think this comes back to like, whenever people ask like, you know, how do I get a language model to do this? You cut, you have to tell it like it's not going to magically know things and so you just tell it that it has kind of in and it, you know, it'll be some like prompt engineering to figure out what exactly you have to tell it and blah blah blah blah blah but you just have a budget keeping agent you tell it has this budget it decides., I don't know if that seems like a decently reasonable approach to me. Yeah, the primary agent pattern is literally just ask, bro. I think that's the main power of some work. I want to take this question. Now you go first. I want to jump back on this question actually because I want to take it a different direction. You go first. Yeah, I just put a button on it then. That sounds like an incredible, like the next step is to start to add like either time or energy or cost-based or token-based constraints. The other thing is that a lot of these questions, John Carmack in his five and a half hour Lex Friedman interview said that he thought there were five ideas you could write on a napkin that would probably get us to AGI. First of all, I want John Carmack in April of 2023 to update that, like how many do you think we have. But the second thing that he said, which I thought was really interesting and true is that a lot of these ideas likely have already been created and explored academically you know so there was a ton of just incredible agent work done even in the 60s, but especially in the mid and late 80s, there were three architectures which are really important to note here. Atlantis BB 1 and sore and there are others as well but my suspicion is that something else that we could do to kind of push this along a lot more quickly is to go back and see okay well you know a lot of this thinking has already been done for us but they didn't have any engineering to be able to implement this and a lot of the science to implement this, okay, what do we have, you know, it looks like we can now do these things in 2023, what in Atlantis BB-1 sore and similar types of architectures, cognitive architectures, you know, can we learn from and can we start to solve some of our, some of these research papers like I hadn't seen many of them but I've seen a lot recently and it's amazing that these people thought of these ideas so long ago and we're in an age that they can be built and it's pretty fascinating That's why it's called research is this searching thing on one thing I wanted to talk on the question, right, on the resource can change, can the baby AGI these limits? Yeah, absolutely, right? You can just tell again, tell it to like here are your constraints. Can you prioritize? Weirdly, when I saw the question I was actually thinking can I get it to prioritize my tasks? And I think what's interesting about that is that if you wanted to build an it wouldn't be baby AGI right I'm just saying can you build a task prioritization that can prioritize your tasks I think that thought exercise it would help you build a better task management agent for the AGI, right? Because you think about what are your constraints? What is the context that you need to feed into your task prioritization to get it to prioritize tasks accurately for you? And if you go through your personal actually like, oh, I need long-term memory, I need an overall summary of what I'm doing, I need like a current milestone. Like as you think about like what would all go into an AI a prompt maybe you know big prompt that could successfully prioritize my task list I think if you ask that question and then step back further up you could kind of generally build a much better task prioritization agent for something like baby AGI. And the reason I bring that up is I think there's a lot of relationship between the way we operate, the way we think our brains and as we think about building these autonomous agents that there's a lot of like cross learning that we should absolutely be talking about. I did want to bring one last major topic in our 10 minutes that we have remaining here, which was the question of like safety. A lot of people are asking about security, like narrowly considered, which is like prompt, you know, is this thing going to pseudo RMRF my machine? Is this thing going to like drain my PayPal account? And then there's also the like broader question of you know what happens when we unleash a whole bunch of like much more capable like web scraping and API hitting bots. So I'd love to hear just like from each sort of panel member in like what they think the path is for what you know, assuaging those concerns that people have. So I'd like to start with Shinyu. Yeah, actually I'm writing something about this. So apparently, you know, when I talked to a lot of people, it seems to be one of the biggest issues because, you know, in the area of lunch model, the biggest harm you can do is generate like a hateful speech, bias speech, loose Asian speech, but now you can generate a virus and release the nuclear weapon or whatever. Anything that a hacker can do potentially the safety considers much more and I don't have like a complete style yet but I think maybe there are two things that we need to consider so why is we might need to guarantee you know the worst case scenario in a sense that we need to decide on action space very carefully so that we could have some guarantee of the worst case scenario. For example, in those web navigation, like you live with action space to only post actions so that it's not going to write on the web pages or you know in Python has started to you know did some of the you know OS packages or so I think that's a way to guarantee worst case scenario. In case of average case or aesthetic case, I think the problem is that humans cannot oversee those things like every moment every time. So this research line of scalable oversight is pretty interesting and I guess the core idea there is you know we eventually need to use agents to only see other agents and then it becomes like a delegation of trust, right? Because I give this much trust to this overseeing agent. And then this overseeing agent will assign some trust value to some of the other agent and it could be a chain. I think it's a very interesting thing and a very important thing to be started and it's a lot of exciting to do. Yeah, so Yoh hey if you want to give your answer? I mean, I think in the short run, right? Like, it's still early. It's definitely not, you know, consumer ready to extend. Oftentimes you'll see technology is pretty rough that definitely can, you know, delete stuff from your computer. We see this all the time. It's not just AI agents, right? But by the time it reaches the average consumer, there's been a nice UI wrapped around it with safety features implemented. I do think the number of non-consumers playing with it is probably higher now. I think due to just code being easier to do, right? Like, I mean, I'm a good example of someone who doesn't really know his way around the terminal, but I'm just copy-pasting things from from from GPT4 and like just messing up my virtual environment and I have no idea what I'm doing so so I do think there are challenges like that that being said I don't think those you know somebody deleting files from their computer is not great, but it's also not like world ending by any means, right? These, you know, as people started driving cars, we added more and more safety features and safety rules so driving cars becomes eventually safer. Now whether right the fact that we drive cars did it destroy cities right did it like did car culture destroy us already right yes or Yes or no? That's an unclear question. I think will autonomous agents destroy, you know, destroy culture? I assume it'll be a similar discussion in the midterm. And then in the long run, like nothing lasts forever, right? I hope that nothing lasts forever refers to autonomous agents or our current conceptions of the world, not like the human species, but I mean. I mean, I, Matt, Matt I mean the universe isn't gonna last forever right as far as we know it going to expand and contract at some point. So when I first started experimenting with these things, I was really, really paranoid. And the I JavaScript module in in cognosis, which, you know, it's an artifact today. I wouldn't recommend using it. But, you know, you can look at it to see a little snapshot in time. And I did what I could to, as Chen you pointed out, to kind of constrain the action surface of the action space that it could operate in by creating a separate repel that I actually talked to via sockets. So it actually wasn't even operating in the same process as the rest of the application. I think that that's probably where this needs to go by default and you know there are different ways that you can isolate another way that I've seen isolation performed you know fairly recently is something else that we actually experimented with too which is putting them into firecracker VMs which are a very very lightweight way of kind of having this isolation and you know by using the pair of virtualization from from Linux. Broad big picture yeah I I think that one of the scariest things to me I posted this I think maybe in October and I was scared to post it in October because I you know as soon as people see this like this is going to happen it didn't or hasn't happened yet surprises me every day when I wake up and I haven't found gpity three or gpity for security scanners just absolutely pummeling the internet with passive scanning or let's say more active scanning because anybody who runs Linux servers or any kind of server on the internet is looked at their log files, knows how actively prodded the services that they post and the service that they have on the internet, you know, have just absolutely massive amounts of daily, weekly, minutely, you know, if you're running in a web server, you're getting hit with 50 different HTTP-based exploits. You know, the smarter ones will try to figure out what kind of software stack you're running. The stupider ones will just try it on everything and you know hope that 0.2% of the servers that it tries are compromiseable. I actually had an experiment, it ran an experiment. I didn't use the open AI stuff for this because I was a little bit paranoid at the time. I didn't want my open AI talk API key to get shut down and it probably wasn't necessary. I thought using Neo X20B for this and I said, okay, you are an expert AI hacker and your friend Matt and E. E. E. A. A. Cuz of E. A. A. You know, challenging you to hack into a machine of theirs. And I don't even know other GPT 4 will do this out of the box. It might be our life shaft out of performing this type of action or you might have to kind of twist its arm to get it to do it as we know that that's possible. But our L. H.F obviously is going to be one facet of this and already is an active facet of this. But that doesn't really help us when we have open source models that we can basically do anything with and fine tune to do anything with. And I think we're going to end up with open source agent fine tune derivations of Loma and alpaca and other types of open source models where it just doesn't matter what kind of like the box is open there's no closing this and at some point in the very near future we're going to have this kind of instead of this passive scanning and kind of really stupid sort of scatter attacks that we all experience running servers and services on the internet. It's about to get really sophisticated. Obviously a lot of topics that we can dive. We have a whole other webinar just on those last three responses, but fortunately we run out of time in this webinar. There is one, there was one question about Langchain that was the highest up voted one that I wanted to hear from Harrison on. But before I kick that one over to Harrison, I did want to thank our panelists both for like being here and answering questions and engaging with each other and with the audience and also just generally for like doing really incredible work and putting it out there for everybody else to learn from. Like not everybody does that. Lots of people like to jealously hide their secrets. And so like you're getting out there putting putting things that other people can build on. It's something that we all like everybody else in the community really appreciate. So thanks to Matt, Yohay, and Sean Yu for everything. And thank you, Charles, for moderating this. That you're always the best. Oh, thanks. It turns out that the yeah. Iteratively improving via criticism is actually the easiest part. The the initial generation is the hard part. Yeah, so the question that came in for Harrison about Langchain. So Langchain, people have maybe heard recently collected some funding, a $10 million seed round, really exciting. But that opens some questions about what, you know, what does that mean for Langchain? Like, how does that become, how does it go from where it is right now, which is this like, you know, open source darling community effort? How does it become a commercial offering? What are the plans around balancing things? Are we talking about a freemium model are we talking about hosting like what kinds of things are you planning this is a question from uh... vincent yeah i mean the the honest answer is we don't really know. You know, LinkedIn started off as a side project as an open source Python package with the goal of making it easy for people to develop language model applications and that's still the primary goal today. And, you know, within within that goal of making any need to develop, there's some stuff that 100% deserves to be open source and there's some stuff that probably will be some type of paid thing. You know, we raised this money like two months ago and we've been exclusively focused on the open source since then so we're so really nothing's changed and I think that's because as we've heard throughout this panel it's like it's still so early in this space, so much is moving. We're not focused on commercialization, we're focused on helping people kind of like build applications with language models, and the best way to do that right now is by developing the open source package and yeah that's kind of how we're thinking about it. So yeah no grand schemes or grand plans we honestly don't know but we'll figure it out along the way. Got it. Great you know I feel like San Francisco's back you know we got we got millions of dollars of funding. We don't have a plan, but we know we got faith in the community. We got faith in the tech. We'll do it. Yeah, that's great to hear. All right, so that's all the time that we have. Thanks everybody for coming for great questions from the audience. Sorry we couldn't answer all of them. There were just, there were probably like 50 questions in there, tried to answer the main ones. But yeah, thanks, thanks everyone for coming and we'll see you all around on Twitter in the AI community. Look forward to seeing the cool stuff everyone here builds. Awesome. Thank you everyone for joining and thank you.