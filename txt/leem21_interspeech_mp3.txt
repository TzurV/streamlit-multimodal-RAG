 Hello everyone my name is Hongin-Linn and today I will introduce our research that focuses on how to deal with the noisy speech for speech emotion recognition in real world environment. So why we do this work? Detecting an emotion from a recorded speech is a useful technology that can be applied to various services like entertainment. Oh, excuse me. Is the raise of point working? Oh, okay. like entertainment or daily health care system. Such useful services can be provided to the human's daily lives by deploying the SEO system on devices like smartphone or smart speakers. However, when deploying the SEL system on real world applications, we have to consider that there are many types of complex background noise in real world environment. People use their devices in various places and in the real world conditions, those environments are highly likely to contain complex types of background noise. Such background noise can degrade the quality of speed signal reading to the SCA system to be hard to detect the emotional information from the recorded speech. So improving the noise robustness for speech emotion recognition is really important to increase the applicability of speech emotional recognition system. What we try to do is to borrow the semi-supervised learning framework to solve this issue. In this framework, the system continuously record the noise speed samples from the noisy target environment. With this collected noise speed samples, we can simultaneously train this SEM model with the noise speed samples from the target environment and the large amount of clean speed samples from the public coppers. We have focused on the emotional attribute which gives us a fine granularity of emotion. We focus on predicting the scores of arousal, balance, and dominance prediction. In this framework we have to consider that such collected noise samples are not likely to contain their emotional label. Entering the emotional labels for all of them takes too much time to collect a large amount of collective noise speed samples. So we have to investigate the approach that can exploit the noise speed samples without requiring their labeling process. What I want to highlight for this approach is a ladder network-based solution. Leadon network uses a denoising auto-encorder architecture consisting of encoder, decoder, and the lateral connection between them. This model can utilize a unlabeled data set during training reading to the model to decrease the domain mismatch between the training set and test set. How can they do that? So the model is simultaneously trained to two different tasks. The first task is prediction task. In this task, the encoder predicts the emotional label of the label data set. To regularize the prediction, they add the arbitrary Gaussian noise for each hidden layer. The second task is a reconstruction task. In the reconstruction task, decoder reconstruct the clean representation from the noisy noisy encoder. To reconstruct the clean representation, they use a retro connection from the noisy encoder and the denoiser in their each layer. So by simultaneously training the model with the prediction and reconstruction task, this model can predict the emotion label as well as containing the information of unlabeled data set. However, in case of applying that the network-based solution to our formulation, we have to consider that there is a background noise in our unlabeled data set. Such background noise should be included in the highest layer of encoder to reconstruct the original speech. Such noisy information can disrupt the emotional prediction so we explore the idea to separate the noisy information from the prediction test. Therefore, we propose the decouple network architecture which decouples the last hidden layer of encoder into two different embeddings. First embedding is reconstruction embedding. Reconstructing is only fed into the decoder and decoder does not construct the emotional related layers. So this scheme allows the reconstruction embedding to contain the information needed for a reconstruction task regardless of the emotional information. The other embedding is emotion embedding. This embedding is only connected to the output layer and this embedding is not included in the reconstruction task. So this scheme allows the emotion embedding to exclusively contain the emotional information. The last of the lower rails are jointly trained with the prediction LA construction test, which is same as the traditional ladder network training. So by using this scheme, we can separate the noisy information from the prediction task as well as keeping the original objective of traditional ladder network training. This model is trained to minimize the summation of prediction and reconstruction laws. To predict the emotional attribute score, we use concurrence correlation coefficient, which is generally called as CCC. In this model, we calculate the agreement between the predictor label and the emotional label. So by using the 1-9cCC as a prediction loss, we can increase the accuracy of the prediction. For reconstruction notes we use a mean scale error between the decoder and the cleaning quarter. So by using this we can allow the decourers to reconstruct the clean representation. We set 1.0 for all of the layers of all of layers reconstruction laws as 1.0 to equally weighted the reconstruction task during the training. So we tested this model with the clean and noisy speech corpus. For clean speech corpus, we use MSP pockets corpus version 1.8. The MSP pockets corpus contains a large amount of spontaneous emotional speech which is collected from the various types of pockets recordings. In the version 1.8 we have more than 113 hours of recording samples. In the pre-processing stage, we remove the segments when they have a background music or overrope speaker in the segments and the samples are selected when their SNL is above 20 decibel so we can use it as a clean speech data set. To use a noise speech data set, we consider that such noise speech data set should be assembled to the real life recordings. In the previous studies, they mostly used, they mostly used method is like, they manually add a noise signal to the clean speed signal. However, their resulting signals are highly likely to contain a single type of noise and their amplitude cannot be buried over the time. Also, to match the duration of clean speed signal, noise signal should be repeated multiple times. Such repeated noise are not likely to occur in real life recording samples and also such non-stationary characteristics are hard to be controlled by using the traditional methods. So what we do is we directly record the MSP Park has corpus in the simulated noisy recording condition. In the single world sound booths we simultaneously play the MSP pockets copers and the noise sound and then we recorded those mixed sounds with the smartphone which is the generally used devices in real world applications. For the noise sound we use a non-capidated radio shows collected from jutto videoU.com. And in this sounds, this sounds contains multiple types of human voice, background music, and various types of sound effects. So such different, such multiple types of sound types can simulate the non-stationarity of the background noise in real life environment. With this setting, we collected different types of recording conditions by bearing the signal to noise ratio. So to do this, we differentiate the distance between the recorder and speech sound and the noise sounds. To calibrate those two distances we collected the one minute samples of speech sound and the noise sound and calculate the estimated signal to noise ratio. So this table shows the two distances and their estimated signal to noise ratio for each recording condition. So according to the estimated signal to noise ratio, we named each recording condition as a 10 d. 5 d. And zero d. be conditions. After collecting the noise speed sample, we transfer the label from the speech corpus, clean speech corpus. Because since the noise sound is not related to the spoken emotional information, we can directly use the original label of the Clean Speech Coopers. So as you can as you see that we use the parallel speech coppers by using MS Parkest corporas and the noisy version of MS ParkPockets corpus. We built the unlabeled data set to train the LADO network-based model. So we collected samples that are not annotated yet. We regarded the unlabel data set as a collected noise speed samples from the target domain so we matched the recording condition of test set and the unlabeled data set. For acoustic feature, we use 6373 dimensions of 2013 compare features, which is mostly used high-level descriptor in speech-emotional recognition field. Our baseline model is Dance Network and Letter Network. Dance Network is a supervised model so we cannot exploit the unlabeled data set during the training of that network. The level network can elaborate the unlabeled data set during their training, but their last hidden layer is not separate into two different embeddings. All the hyper-parameters for training and there are a number of nodes and layers are identical to each other in baseline and the decoupled network. You can see the details settings in our paper. So here is the result. We summarize the average concurrence correlation coefficient over 20 trials for each model and each recording conditions. You can first see that the performance decreases as the signal to noise ratio of the recording condition gets down. So we can say that there is a detrimental effect of background noise to the emotional prediction. As we compare the letter network with the desk network performance, in the noisy recording condition we can see that there is some there is an improvement of the of of letter network in arousal and dominance prediction. So we can say, so this result shows that the semi-superwise learning framework can increase the performance in the noisy recording condition. Such improvement can be further increased when we use the decoupled network architecture. In the prediction of arousal, there was an improvement by 11.4%, 8.4% and 10.2% for each recording condition. Such trend is similar to the dominance prediction. So we can see that the decouple network can improve the performance in new easily according condition. But however, when you look at the balance prediction model, the improvement is not clearer comparing with the other prediction tasks. So to get an idea of this result, I will introduce one of our analysis. So in this analysis we try to reconstruct the info feature by using emotion embedding instead of using reconstruction embedding. The reason why is that we assume that emotion embedding will have less information for reconstruction tasks than the reconstruction embedding. So what we do is after training we cut the connection between the reconstruction embedding and the decoder and reconnected to the emotion embedding. And then we compare the reconstruction laws of input features by using emotion embedding and reconstruction embedding. So here is the result. This graph shows the mean-scale error of using emotion embedding and reconstruction embedding. The black bar shows the loss of using emotion embedding. And gray bar shows the loss of using reconstruction embedding. So you can see that in the arousal and dominance model the loss of using emotion embedding shows is higher than the loss of using emotion embedding is higher than the loss of using reconstruction embedding. So we can say that there are more information of reconstruction in reconstruction embedding comparing with the emotion embedding. But however, when you look at the balance prediction model, the difference is not much clearer comparing with the other conditions. We can say that this result means that the embedding is not well separated in the balance prediction model for the lower as-than-a condition. So we assume that this result is highly related to the less improvement of the couple of the couple of network in the balance prediction model. So our plan is to impose more constraints to further decolate between emotion and reconstruction embedding to further improve the decoupled network performance. So in this presentation we propose the decouple network which can improve the performance of speech emotional recognition in noisy recording condition. We also introduced the noisy version of MSP pockets corpus, which can simulate the noisy speed samples obtained from the real life applications. We plan to include the noisy version of MSP Parkhouse Copers in our future release of MSP Parkers Copers. For a clean speech you can you can still use use it by by sending the requests to us. So if you're really, if you're interested in it, please visit our website and send us a request. Thanks for attending this presentation and I'll open the Q&A session now. Thank you very much. We have time for questions now from the audience. It was really interesting. So I have basically two questions. So the first one is for MSP podcast, is it annotated, I mean in terms of violence or also and dominance, like with some rate, because usually it's like the like plot or something. I mean, if you annotate it as a Valian, Roso and so on the audio you have some time steps to annotate so at which rate do you have for one second for example? How many annotations in one second. Actually, among the countenance recordings, we segmented the samples as one sentence, and then we asked to record us to annotate emotional attribute scores for one sentence. So I would say that those scores are from the total of the entire of the one speed sentence. Okay. Sentence is like, I don't know, five seconds or so? Yeah mostly like a five seconds and there are ranges like 2.75 seconds to 11 seconds. Okay so I mean so on the second question yeah thank you so the second question is so the compare the 2013 feature set is usually calculated for all the audio yes right, right. So we use one representation per one sentence. Ah, yeah, okay, so now I see. And so you calculated CCC for hold the sentences, I mean the yeah hold the sentences so there are there are one predatory score for one sentence and there are one emotional label for one sentence so we only we calculate the CCC between those two scalar values. Okay, I see. Yeah, thank you. Thank you. So it was another question. I might have sort of misunderstood the presentation partly but did you use only all the sort of noisy recordings where all of them which you used were created using the method you presented where you added the noise artificially? Because of course in the noisy environment people speak differently. There's Lombard effect people and this of course might tremendously interact with the reading of sort of emotional state because you speak louder and slower and all that. That's a very good point. So I will say that, yeah, you're right. So environmental condition can affect the speaker's speaking style. But I would say that how do you collect the data set to simulate the Lombard speech? So we can think about the framework like recruiter workers and put the headphones like playing the noise sound and act emotional speech. But however in this case that speech corporates will be highly likely to be active speech corporates. What we want to do is deal with the spontaneous emotional speech, because as I said, we want to increase the performance in real world recording conditions, real world environment. And in the real world environment and in the real world applications the most of the collective speed samples are highly likely to be a spontaneous emotional speech. So we have to select the two options so do you want to simulate the lumba speech samples or do you want to simulate the spontaneous emotional speech samples so in the in the among those two options we select the spontaneous emotional speech. Yeah, thank you. Yeah, thank you very much. Okay, there are two questions from the chat. So the first one from Hirah Tamial, Carnegie Mellon University. How different are the emotion and the reconstruction embedding? What can be done to separate the two embedding so they learn separate things? So as I understand this question that what will be your constraints to further decoolate those two embeddings? So we think about the including the orthogonal loss between those two embeddings like cosine distance between emotion and the construction embedding or we can also apply the gradient rebossal layer to remove the to further remove the emotional information in the construction embedding that's what we plan for our future study. Okay and the second question from C1-Zang West Lake University. Very impressive work combining speed separation and S-E-R, could you please give more details about a semi-supervised learning mechanism in your proposed network? Details on the semi-supervised learning framework. So let's say about the ladder network. So let's say that the network is, as I said, the network is simultaneously trained with the prediction and the construction test. The reason why I say that Lettle Network can minimize the domain mismatch between training set and test set is because that we can use the unlabeled data set collected from the target environment. So during the training of semi-superized framework we can use a label set for the prediction task and we can use a label and unlabeled data set for the reconstruction test. So you can think like that we combine the Rabel and unlabel data set during the training of the LIDAW network-based model. Okay, thank you.